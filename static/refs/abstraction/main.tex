\section*{\Huge Abstract}
Large Language Models (LLMs) have revolutionized natural language processing, showcasing remarkable capabilities in tasks such as text generation, translation, and question answering. However, their reliance on massive text datasets for training often results in factual inaccuracies, inconsistencies, and limitations in accessing specific knowledge domains. This inherent constraint hinders their performance in scenarios demanding precise factual accuracy and comprehensive knowledge retrieval. To address these challenges, this project explores the integration of knowledge graphs, structured representations of information, with LLMs. We analyze existing retrieval-augmented generation (RAG) techniques, including Text-Based RAG, which retrieves relevant documents from external corpora, and Knowledge Graph-Based RAG (KG-RAG), which leverages knowledge graph information to enrich LLM responses. While these methods demonstrate improvements, they often suffer from retrieval bottlenecks and challenges in incorporating external knowledge into the LLM's reasoning process. To overcome these limitations, we propose KEALLM - a Knowledge Graph Embedding Augmented Large Language Model. Unlike other RAG approaches, KEALLM directly incorporates knowledge graph embeddings into the LLM architecture, enabling a more integrated and efficient knowledge utilization. To evaluate the effectiveness of KEALLM, we conduct experiments on one-hop question answering tasks, comparing its performance against standard LLMs, Text-Based RAG, and KG-RAG. Our results demonstrate that KEALLM consistently outperforms standard LLMs and Text-Based RAG, achieving comparable performance to KG-RAG while offering a more streamlined and efficient architecture. This project highlights the significant potential of integrating knowledge graph embeddings with LLMs. KEALLM offers a promising avenue for developing more knowledgeable, accurate, and reliable LLMs, paving the way for advancements in various domains, including question answering, dialogue systems, and knowledge-intensive Natural Language Processing (NLP) applications. Further research will focus on refining KEALLM's architecture, exploring different knowledge graph embedding techniques, and evaluating its performance on more complex NLP tasks.
