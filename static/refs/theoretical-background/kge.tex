\section{Knowledge graph embedding}
Knowledge graph embedding (KGE) aims to represent entities and relations in a knowledge graph as low-dimensional vectors, capturing their semantic relationships. This section focuses on two prominent approaches for KGE: translation models and pretrained language models.
\subsection{Translation models}
Translation models, pioneered by TransE \cite{NIPS2013_1cecc7a7}, are based on the idea of representing relations as translations in the embedding space. For a triple $(h, r, t)$ representing head entity $h$, relation $r$, and tail entity $t$, TransE enforces the following relation:
\begin{align}
\mathbf{h} + \mathbf{r} \approx \mathbf{t}
\end{align}
where $\mathbf{h}$, $\mathbf{r}$, and $\mathbf{t}$ are the embedding vectors of $h$, $r$, and $t$, respectively. This implies that the embedding of the tail entity should be close to the embedding of the head entity translated by the relation vector. TransE utilizes a margin-based ranking loss to encourage correct triples to have lower scores than incorrect ones.
Several extensions to TransE have been proposed to address its limitations in modeling complex relations like one-to-many, many-to-one, and many-to-many. These include TransH \cite{Wang_Zhang_Feng_Chen_2014}, TransR \cite{Lin_Liu_Sun_Liu_Zhu_2015}, and TransD \cite{ji-etal-2015-knowledge}, which introduce relation-specific hyperplanes, projection matrices, and dynamic mapping matrices, respectively, to handle diverse relation patterns.
\subsection{Pretrained language models}
Pretrained Language Models (PLMs), such as BERT \cite{devlin-etal-2019-bert} and RoBERTa \cite{liu2019roberta}, have demonstrated remarkable capabilities in capturing contextualized word representations. Recently, PLMs have been adapted for knowledge graph embedding by treating entities and relations as special tokens within the language model.
KG-BERT \cite{yao2019kg} utilizes BERT for link prediction by transforming it into a masked entity prediction task. Given an incomplete triple $(h, r, ?)$, KG-BERT constructs an input sequence:
\begin{align*}
x_t = \text{\texttt{[CLS]}} h \text{\texttt{[SEP}]} r \text{\texttt{[SEP]}}
\end{align*}
The model then predicts the masked entity by ranking the probability of each entity in the knowledge graph:
\begin{align}
p(t|x_t) = p(\text{\texttt{[CLS]}} = t|x_t;\Phi)
\end{align}
where $\Phi$ represents the model's parameters. KG-BERT is trained using a cross-entropy loss, encouraging the model to assign high probability to the correct tail entity.
This approach effectively leverages the rich semantic information captured by PLMs to learn knowledge graph embeddings. Moreover, it allows for incorporating textual descriptions associated with entities and relations, further enhancing the expressiveness of the embeddings.
By utilizing translation models or pretrained language models, knowledge graph embedding provides a powerful framework for representing and reasoning over knowledge graphs, enabling various downstream tasks such as link prediction, question answering, and recommendation systems.
