\section{Direct preference optimization}
Direct preference optimization (DPO) is a learning paradigm that fine-tunes language models to align with user preferences, as introduced in \cite{rafailov2024direct}. Unlike traditional Reinforcement Learning from Human Feedback (RLHF) methods \cite{NIPS2017_d5e2c0ad} that require training a separate reward model and then using reinforcement learning to optimize the policy, DPO directly optimizes the policy to match human preferences using a simple classification objective.
The key insight of DPO is that the optimal policy for a KL-regularized reward maximization objective can be expressed in closed form. This allows us to directly parameterize the policy in a way that implicitly encodes the reward function, bypassing the need for a separate reward model.
Consider a language model policy $\pi_\theta(y|x)$ that generates response $y$ given prompt $x$ and is parameterized by $\theta$. We want to optimize this policy to maximize a reward function $r(x,y)$ while staying close to a reference policy $\pi_{ref}(y|x)$ (usually the initial supervised fine-tuned model). The standard RLHF objective is:
\begin{align}
\max_{\pi_\theta} \mathbb{E}_{x \sim D, y \sim \pi_\theta(y|x)} [ r(x, y)] - \beta D_{KL}[\pi_\theta(y | x) || \pi_{ref}(y | x)]
\end{align}
where $\beta$ controls the strength of the KL penalty.
The optimal solution to this objective has the following form:
\begin{align}
\pi_r(y | x) = \frac{1}{Z(x)} \pi_{ref}(y | x) \exp \left( \frac{1}{\beta} r(x, y) \right)
\end{align}
where $Z(x)$ is the partition function.
DPO leverages this closed-form solution by rearranging it to express the reward function in terms of the optimal policy and the reference policy:
\begin{align}
r(x, y) = \beta \log \frac{\pi_r(y | x)}{\pi_{ref}(y | x)} + \beta \log Z(x)
\end{align}
Substituting this expression for the reward function into a preference model (like the Bradley-Terry model) allows us to express the probability of human preferences directly in terms of the policy and the reference policy. Crucially, the partition function $Z(x)$ cancels out, leaving a tractable objective.
For the Bradley-Terry model, the probability of preferring response $y_w$ over $y_l$ given prompt $x$ is:
\begin{align}
p(y_w \succ y_l | x) = \sigma \left( r(x, y_w) - r(x, y_l) \right)
\end{align}
Substituting the reparameterized reward function and simplifying, we get:
\begin{align}
p(y_w \succ y_l | x) = \sigma \left( \beta \log \frac{\pi_r(y_w | x)}{\pi_{ref}(y_w | x)} - \beta \log \frac{\pi_r(y_l | x)}{\pi_{ref}(y_l | x)} \right)
\end{align}
This leads to the DPO loss function, which is a simple binary cross-entropy loss:
\begin{align}
\mathcal{L}_{DPO} = -\mathbb{E}_{(x, y_w, y_l) \sim D} \log \sigma \left( \beta \left( \log \frac{\pi_\theta(y_w | x)}{\pi_{ref}(y_w | x)} - \log \frac{\pi_\theta(y_l | x)}{\pi_{ref}(y_l | x)} \right) \right)
\end{align}
where $D$ is the dataset of human preference data.
By minimizing this loss, we directly optimize the policy $\pi_\theta$ to match the observed preferences, implicitly learning a reward function that aligns with those preferences. This eliminates the need for a separate reward model and RL, making DPO a simpler and more efficient approach compared to traditional RLHF methods.